import numpy as np
import torch
import torch.nn as nn
from torchvision import transforms

from .engine.transfer_trainer import TransferTrainer
from .engine.trainer import Trainer
from .datasets import Searchstims, VOCDetection
from .utils.general import make_save_path
from .utils.transforms import normalize, VOCTransform

# declare as a constant because also referenced by munge.VSD_results_df
VSD_PAD_SIZE = 500


def train(csv_file,
          dataset_type,
          net_name,
          number_nets_to_train,
          epochs_list,
          batch_size,
          random_seed,
          save_path,
          root=None,
          pad_size=VSD_PAD_SIZE,
          method='transfer',
          num_classes=2,
          learning_rate=None,
          base_learning_rate=1e-20,
          new_learn_rate_layers='fc8',
          new_layer_learning_rate=0.001,
          freeze_trained_weights=True,
          loss_func='CE',
          optimizer='SGD',
          use_val=True,
          val_epoch=1,
          summary_step=None,
          patience=None,
          checkpoint_epoch=10,
          save_acc_by_set_size_by_epoch=False,
          num_workers=4,
          data_parallel=False):
    """train neural networks to perform visual search task.

    Parameters
    ----------
    csv_file : str
        name of .csv file containing prepared data sets.
        Generated by searchnets.data.split function from a csv created by the searchstims library.
    dataset_type : str
        one of {'searchstims', 'VSD'}. Specifies whether dataset is images generated by searchstims package, or
        images from Pascal-VOC dataset that were used to create the Visual Search Difficulty 1.0 dataset.
    net_name : str
        name of convolutional neural net architecture to train.
        One of {'alexnet', 'VGG16'}
    number_nets_to_train : int
        number of training "replicates"
    epochs_list : list
        of training epochs. Replicates will be trained for each
        value in this list. Can also just be one value, but a list
        is useful if you want to test whether effects depend on
        number of training epochs.
    batch_size : int
        number of samples in a batch of training data
    random_seed : int
        to seed random number generator
    save_path : str
        path to directory where checkpoints and train models were saved
    root : str
        path to dataset root. Used with VOCDetection dataset to specify where VOC data was/should be downloaded
        to. (Note that download will take a while.)
    method : str
        training method. One of {'initialize', 'transfer'}.
        'initialize' means randomly initialize all weights and train the
        networks "from scratch".
        'transfer' means perform transfer learning, using weights pre-trained
        on imagenet.
        Default is 'transfer'.

    Other Parameters
    ----------------
    num_classes : int
        number of classes. Default is 2 (target present, target absent).
    base_learning_rate : float
        Applied to layers with weights loaded from training the
        architecture on ImageNet. Should be a very small number
        so the trained weights don't change much.
    freeze_trained_weights : bool
        if True, freeze weights in any layer not in "new_learn_rate_layers".
        These are the layers that have weights pre-trained on ImageNet.
        Default is False. Done by simply not applying gradients to these weights,
        i.e. this will ignore a base_learning_rate if you set it to something besides zero.
    new_learn_rate_layers : list
        of layer names whose weights will be initialized randomly
        and then trained with the 'new_layer_learning_rate'.
    new_layer_learning_rate : float
        Applied to `new_learn_rate_layers'. Should be larger than
        `base_learning_rate` but still smaller than the usual
        learning rate for a deep net trained with SGD,
        e.g. 0.001 instead of 0.01
    loss_func : str
        type of loss function to use. One of {'CE', 'InvDPrime', 'triplet'}. Default is 'CE',
        the standard cross-entropy loss. 'InvDPrime' is inverse D prime. 'triplet' is triplet loss
        used in face recognition and biometric applications.
    optimizer : str
        optimizer to use. One of {'SGD', 'Adam', 'AdamW'}.
    save_acc_by_set_size_by_epoch : bool
        if True, compute accuracy on training set for each epoch separately
        for each unique set size in the visual search stimuli. These values
        are saved in a matrix where rows are epochs and columns are set sizes.
        Useful for seeing whether accuracy converges for each individual
        set size. Default is False.
    use_val : bool
        if True, use validation set.
    val_epoch : int
        if not None, accuracy on validation set will be measured every `val_epoch` epochs. Default is None.
    summary_step : int
        Step on which to write summaries to file. Each minibatch is counted as one step, and steps are counted across
        epochs. Default is None.
    patience : int
        if not None, training will stop if accuracy on validation set has not improved in `patience` steps
    num_workers : int
        number of workers used by torch.DataLoaders. Default is 4.
    data_parallel : bool
        if True, use torch.nn.dataparallel to train network on multiple GPUs. Default is False.

    Returns
    -------
    None
    """
    if use_val and val_epoch is None or val_epoch < 1 or type(val_epoch) != int:
        raise ValueError(
            f'invalid value for val_epoch: {val_epoch}. Validation epoch must be positive integer'
        )

    if use_val is False and patience is not None:
        raise ValueError('patience argument only works with a validation set')

    if patience is not None:
        if type(val_epoch) != int or patience < 1:
            raise TypeError('patience must be a positive integer')

    if type(epochs_list) is int:
        epochs_list = [epochs_list]
    elif type(epochs_list) is list:
        pass
    else:
        raise TypeError("'EPOCHS' option in 'TRAIN' section of config.ini file parsed "
                        f"as invalid type: {type(epochs_list)}")

    if random_seed:
        np.random.seed(random_seed)  # for shuffling in batch_generator
        torch.manual_seed(random_seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False

    if torch.cuda.is_available():
        device = torch.device('cuda')
    else:
        device = torch.device('cpu')

    if dataset_type == 'VSD':
        trainset = VOCDetection(root=root,
                                csv_file=csv_file,
                                image_set='trainval',
                                split='train',
                                download=True,
                                transforms=VOCTransform(pad_size=pad_size)
                                )
        if use_val:
            valset = VOCDetection(root=root,
                                  csv_file=csv_file,
                                  image_set='trainval',
                                  split='val',
                                  download=True,
                                  transforms=VOCTransform(pad_size=pad_size)
                                  )
        else:
            valset = None

    elif dataset_type == 'searchstims':
        trainset = Searchstims(csv_file=csv_file,
                               split='train',
                               transform=transforms.Compose(
                                   [transforms.ToTensor(), normalize]
                               ))

        if use_val:
            valset = Searchstims(csv_file=csv_file,
                                 split='val',
                                 transform=transforms.Compose([transforms.ToTensor(), normalize]))
        else:
            valset = None

    if save_acc_by_set_size_by_epoch:
        if dataset_type == 'VSD':
            raise ValueError(
                'dataset type is VSD but save_acc_by_set_size_by_epoch was set to True;'
                'can only measure accuracy by set size with searchstims stimuli, not VSD dataset'
            )
        elif dataset_type == 'searchstims':
            trainset_set_size = Searchstims(csv_file=csv_file,
                                            split='train',
                                            transform=transforms.Compose(
                                                [transforms.ToTensor(), normalize]),
                                            return_set_size=True)
    else:
        trainset_set_size = None

    if dataset_type == 'searchstims':
        apply_sigmoid = False
    elif dataset_type == 'VSD':
        apply_sigmoid = True  # for multi-label prediction

    if loss_func == 'CE':
        criterion = nn.CrossEntropyLoss()
    elif loss_func == 'BCE':
        criterion = nn.BCELoss()

    for epochs in epochs_list:
        print(f'training {net_name} model for {epochs} epochs')
        for net_number in range(1, number_nets_to_train + 1):
            save_path_this_net = make_save_path(save_path, net_name, net_number, epochs)
            if method == 'transfer':
                trainer = TransferTrainer.from_config(net_name=net_name,
                                                      trainset=trainset,
                                                      new_learn_rate_layers=new_learn_rate_layers,
                                                      freeze_trained_weights=freeze_trained_weights,
                                                      base_learning_rate=base_learning_rate,
                                                      new_layer_learning_rate=new_layer_learning_rate,
                                                      save_path=save_path_this_net,
                                                      num_classes=num_classes,
                                                      apply_sigmoid=apply_sigmoid,
                                                      criterion=criterion,
                                                      optimizer=optimizer,
                                                      save_acc_by_set_size_by_epoch=save_acc_by_set_size_by_epoch,
                                                      trainset_set_size=trainset_set_size,
                                                      batch_size=batch_size,
                                                      epochs=epochs,
                                                      use_val=use_val,
                                                      valset=valset,
                                                      val_epoch=val_epoch,
                                                      patience=patience,
                                                      checkpoint_epoch=checkpoint_epoch,
                                                      summary_step=summary_step,
                                                      device=device,
                                                      num_workers=num_workers,
                                                      data_parallel=data_parallel)
            elif method == 'initialize':
                trainer = Trainer.from_config(net_name=net_name,
                                              trainset=trainset,
                                              save_path=save_path_this_net,
                                              num_classes=num_classes,
                                              apply_sigmoid=apply_sigmoid,
                                              criterion=criterion,
                                              optimizer=optimizer,
                                              learning_rate=learning_rate,
                                              save_acc_by_set_size_by_epoch=save_acc_by_set_size_by_epoch,
                                              trainset_set_size=trainset_set_size,
                                              batch_size=batch_size,
                                              epochs=epochs,
                                              use_val=use_val,
                                              valset=valset,
                                              val_epoch=val_epoch,
                                              patience=patience,
                                              checkpoint_epoch=checkpoint_epoch,
                                              summary_step=summary_step,
                                              device=device,
                                              num_workers=num_workers,
                                              data_parallel=data_parallel)

            trainer.train()
