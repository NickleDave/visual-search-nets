
@article{wolfeFiveFactorsThat2017,
  title = {Five Factors That Guide Attention in Visual Search},
  volume = {1},
  issn = {23973374},
  doi = {10.1038/s41562-017-0058},
  abstract = {How do we find what we are looking for? Even when the desired target is in the current field of view, we need to search because fundamental limits on visual processing make it impossible to recognize everything at once. Searching involves directing atten- tion to objects that might be the target. This deployment of attention is not random. It is guided to the most promising items and locations by five factors discussed here: bottom-up salience, top-down feature guidance, scene structure and meaning, the pre- vious history of search over timescales ranging from milliseconds to years, and the relative value of the targets and distractors. Modern theories of visual search need to incorporate all five factors and specify how these factors combine to shape search behaviour. An understanding of the rules of guidance can be used to improve the accuracy and efficiency of socially important search tasks, from security screening to medical image perception.},
  number = {3},
  journal = {Nature Human Behaviour},
  author = {Wolfe, Jeremy M. and Horowitz, Todd S.},
  year = {2017},
  pages = {1-8},
  file = {C:\\Users\\Seymour Snyder\\Documents\\Mendeley Desktop\\Wolfe, Horowitz - 2017 - Five factors that guide attention in visual search.pdf},
  pmid = {9384378}
}

@article{duncanVisualSearchStimulus1989,
  title = {Visual {{Search}} and {{Stimulus Similarity}}},
  volume = {96},
  number = {3},
  author = {Duncan, John and Humphreys, Glyn W},
  year = {1989},
  pages = {433-458},
  file = {C:\\Users\\Seymour Snyder\\Documents\\Mendeley Desktop\\Duncan, Humphreys - 1989 - Visual Search and Stimulus Similarity.pdf}
}

@article{poderCapacityLimitationsVisual2017,
  title = {Capacity Limitations of Visual Search in Deep Convolutional Neural Network},
  abstract = {Deep convolutional neural networks follow roughly the architecture of biological visual systems, and have shown a performance comparable to human observers in object recognition tasks. In this study, I test a pre-trained deep neural network in some classic visual search tasks. The results reveal a qualitative difference from human performance. It appears that there is no difference between searches for simple features that pop out in experiments with humans, and for feature configurations that exhibit strict capacity limitations in human vision. Both types of stimuli reveal moderate capacity limitations in the neural network tested here.},
  author = {Poder, Endel},
  year = {2017},
  keywords = {attention,DARPA,L2M,visual search},
  file = {C:\\Users\\Seymour Snyder\\Documents\\Mendeley Desktop\\Poder - 2017 - Capacity limitations of visual search in deep convolutional neural network.pdf}
}

@article{khaligh-razaviDeepSupervisedNot2014,
  title = {Deep Supervised, but Not Unsupervised, Models May Explain {{IT}} Cortical Representation},
  volume = {10},
  number = {11},
  journal = {PLoS computational biology},
  author = {{Khaligh-Razavi}, Seyed-Mahdi and Kriegeskorte, Nikolaus},
  year = {2014},
  pages = {e1003915}
}

@article{geislerIdealObserverAnalysis2003,
  title = {Ideal Observer Analysis},
  volume = {10},
  number = {7},
  journal = {The visual neurosciences},
  author = {Geisler, Wilson S.},
  year = {2003},
  pages = {12-12}
}

@article{kriegeskorteDeepNeuralNetworks2015,
  title = {Deep Neural Networks: A New Framework for Modeling Biological Vision and Brain Information Processing},
  volume = {1},
  journal = {Annual review of vision science},
  author = {Kriegeskorte, Nikolaus},
  year = {2015},
  pages = {417-446}
}

@inproceedings{krizhevskyImagenetClassificationDeep2012,
  title = {Imagenet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2012},
  pages = {1097-1105}
}

@incollection{wolfeVisualSearch1998,
  address = {Hove, England},
  title = {Visual Search},
  isbn = {978-0-86377-812-4 978-0-86377-813-1},
  abstract = {Discusses visual search tasks where the object is visible in the current field of view. The author discusses paradigmatic issues, such as accuracy methods, interpretation of search results, and the description of search performance. The author next describes the properties of preattentive processing, the processing of stimuli that occurs before attention is deployed to an item in a search task. The author then discusses the use of preattentive information by subsequent processes, including stimulus-driven and user-driven processes. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  booktitle = {Attention},
  publisher = {{Psychology Press/Erlbaum (UK) Taylor \& Francis}},
  author = {Wolfe, Jeremy M.},
  year = {1998},
  keywords = {Attention,Cognitive Processes,Visual Search},
  pages = {13-73},
  file = {/home/art/Zotero/storage/B5LQBDSI/1998-07791-001.html}
}

@article{wolfeReactionTimeDistributions2010,
  title = {Reaction Time Distributions Constrain Models of Visual Search},
  volume = {50},
  number = {14},
  journal = {Vision research},
  author = {Wolfe, Jeremy M. and Palmer, Evan M. and Horowitz, Todd S.},
  year = {2010},
  pages = {1304-1311}
}

@article{bergenRapidDiscriminationVisual1983,
  title = {Rapid Discrimination of Visual Patterns},
  number = {5},
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  author = {Bergen, James R. and Julesz, Bela},
  year = {1983},
  pages = {857-863}
}

@article{geislerModelsOvertAttention2011,
  title = {Models of Overt Attention},
  journal = {Oxford handbook of eye movements},
  author = {Geisler, Wilson S. and Cormack, Lawrence K.},
  year = {2011},
  pages = {439-454}
}

@article{palmerPsychophysicsVisualSearch2000,
  title = {The Psychophysics of Visual Search},
  volume = {40},
  number = {10-12},
  journal = {Vision research},
  author = {Palmer, John and Verghese, Preeti and Pavel, Misha},
  year = {2000},
  pages = {1227-1268}
}

@article{ecksteinVisualSearchRetrospective2011,
  title = {Visual Search: {{A}} Retrospective},
  volume = {11},
  number = {5},
  journal = {Journal of vision},
  author = {Eckstein, Miguel P.},
  year = {2011},
  pages = {14-14}
}

@article{ecksteinLowerVisualSearch1998a,
  title = {The Lower Visual Search Efficiency for Conjunctions Is Due to Noise and Not Serial Attentional Processing},
  volume = {9},
  number = {2},
  journal = {Psychological Science},
  author = {Eckstein, Miguel P.},
  year = {1998},
  pages = {111-118}
}

@article{ecksteinSignalDetectionModel2000,
  title = {A Signal Detection Model Predicts the Effects of Set Size on Visual Search Accuracy for Feature, Conjunction, Triple Conjunction, and Disjunction Displays},
  volume = {62},
  number = {3},
  journal = {Perception \& psychophysics},
  author = {Eckstein, Miguel P. and Thomas, James P. and Palmer, John and Shimozaki, Steven S.},
  year = {2000},
  pages = {425-451}
}

@article{treismanFeatureintegrationTheoryAttention1980,
  title = {A Feature-Integration Theory of Attention},
  volume = {12},
  number = {1},
  journal = {Cognitive psychology},
  author = {Treisman, Anne M. and Gelade, Garry},
  year = {1980},
  pages = {97-136}
}

@article{wolfeGuidedSearchAlternative1989,
  title = {Guided Search: An Alternative to the Feature Integration Model for Visual Search.},
  volume = {15},
  number = {3},
  journal = {Journal of Experimental Psychology: Human perception and performance},
  author = {Wolfe, Jeremy M. and Cave, Kyle R. and Franzel, Susan L.},
  year = {1989},
  pages = {419}
}

@article{wolfeGuidedSearchRevised1994,
  title = {Guided Search 2.0 a Revised Model of Visual Search},
  volume = {1},
  number = {2},
  journal = {Psychonomic bulletin \& review},
  author = {Wolfe, Jeremy M.},
  year = {1994},
  pages = {202-238}
}

@article{wolfeWhatCanMillion1998,
  title = {What Can 1 Million Trials Tell Us about Visual Search?},
  volume = {9},
  number = {1},
  journal = {Psychological Science},
  author = {Wolfe, Jeremy M.},
  year = {1998},
  pages = {33-39}
}

@article{palmerSignalDetectionEvidence2011,
  title = {Signal Detection Evidence for Limited Capacity in Visual Search},
  volume = {73},
  number = {8},
  journal = {Attention, Perception, \& Psychophysics},
  author = {Palmer, Evan M. and Fencsik, David E. and Flusberg, Stephen J. and Horowitz, Todd S. and Wolfe, Jeremy M.},
  year = {2011},
  pages = {2413-2424}
}

@article{yaminsUsingGoaldrivenDeep2016,
  title = {Using Goal-Driven Deep Learning Models to Understand Sensory Cortex},
  volume = {19},
  issn = {1546-1726},
  doi = {10.1038/nn.4244},
  abstract = {Fueled by innovation in the computer vision and artificial intelligence communities, recent developments in computational neuroscience have used goal-driven hierarchical convolutional neural networks (HCNNs) to make strides in modeling neural single-unit and population responses in higher visual cortical areas. In this Perspective, we review the recent progress in a broader modeling context and describe some of the key technical innovations that have supported it. We then outline how the goal-driven HCNN approach can be used to delve even more deeply into understanding the development and organization of sensory cortical processing.},
  language = {eng},
  number = {3},
  journal = {Nature Neuroscience},
  author = {Yamins, Daniel L. K. and DiCarlo, James J.},
  month = mar,
  year = {2016},
  keywords = {Animals,Goals,Humans,Learning,Models; Neurological,Neural Networks (Computer),Somatosensory Cortex,Visual Cortex},
  pages = {356-365},
  pmid = {26906502}
}

@article{yaminsPerformanceoptimizedHierarchicalModels2014,
  title = {Performance-Optimized Hierarchical Models Predict Neural Responses in Higher Visual Cortex},
  volume = {111},
  number = {23},
  journal = {Proceedings of the National Academy of Sciences},
  author = {Yamins, Daniel LK and Hong, Ha and Cadieu, Charles F. and Solomon, Ethan A. and Seibert, Darren and DiCarlo, James J.},
  year = {2014},
  pages = {8619--8624},
  file = {/home/art/Zotero/storage/WY2INNG8/Yamins et al. - 2014 - Performance-optimized hierarchical models predict .pdf;/home/art/Zotero/storage/XYGIZ26G/8619.html}
}

@article{peelenAttentionRealWorld2014,
  title = {Attention in the Real World: Toward Understanding Its Neural Basis},
  volume = {18},
  shorttitle = {Attention in the Real World},
  number = {5},
  journal = {Trends in cognitive sciences},
  author = {Peelen, Marius V. and Kastner, Sabine},
  year = {2014},
  pages = {242--250},
  file = {/home/art/Zotero/storage/5LZDSZCR/PMC4908952.html;/home/art/Zotero/storage/WRLRWZTC/S1364661314000473.html}
}

@book{nicholsonVisualsearchnets2019a,
  title = {Visual-Search-Nets},
  author = {Nicholson, David},
  year = {2019},
  doi = {10.5281/zenodo.3171396}
}

@inproceedings{dengImagenetLargescaleHierarchical2009,
  title = {Imagenet: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {Imagenet},
  booktitle = {2009 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  publisher = {{Ieee}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  pages = {248--255},
  file = {/home/art/Zotero/storage/UUHRG4JC/Deng et al. - 2009 - Imagenet A large-scale hierarchical image databas.pdf}
}

@inproceedings{yosinskiHowTransferableAre2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  pages = {3320--3328},
  file = {/home/art/Zotero/storage/RDPQKFJJ/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf;/home/art/Zotero/storage/IFHQVBFT/5347-how-transferable-are-features-in-deep-nâ€¦.html}
}


